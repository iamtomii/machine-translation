{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "transformer_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVOKeezWPsSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d54cd4-7bef-45ac-a4d4-36e2c1cbba53"
      },
      "source": [
        "! pip -q install torchtext==0.6.0\n",
        "! pip -q install pyvi \n",
        "! pip -q install https://github.com/trungtv/vi_spacy/raw/master/packages/vi_spacy_model-0.2.1/dist/vi_spacy_model-0.2.1.tar.gz\n",
        "! python -m spacy link vi_spacy_model vi_spacy_model\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 71kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.5MB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 747kB 54.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 42.3MB 76kB/s \n",
            "\u001b[?25h  Building wheel for vi-spacy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/vi_spacy_model -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/vi_spacy_model\n",
            "You can now load the model via spacy.load('vi_spacy_model')\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gvN64qvNQIS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9da_ZuSNQIW"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "    \n",
        "# Embedder(100, 512)(torch.LongTensor([1,2,3,4])).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP64KizDNQIa"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=200, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        \n",
        "        # Bảng pe mình vẽ ở trên \n",
        "        for pos in range(max_seq_length):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
        "        pe = pe.unsqueeze(0)        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x*math.sqrt(self.d_model)\n",
        "        seq_length = x.size(1)\n",
        "        \n",
        "        pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
        "        \n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        # cộng embedding vector với pe \n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "# PositionalEncoder(512)(torch.rand(5, 30, 512)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJMcGuUNQId"
      },
      "source": [
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    q: batch_size x head x seq_length x d_model\n",
        "    k: batch_size x head x seq_length x d_model\n",
        "    v: batch_size x head x seq_length x d_model\n",
        "    mask: batch_size x 1 x 1 x seq_length\n",
        "    output: batch_size x head x seq_length x d_model\n",
        "    \"\"\"\n",
        "\n",
        "    # attention score được tính bằng cách nhân q với k\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "    # xong rồi thì chuẩn hóa bằng softmax\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "    \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output, scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQ4C3EENQIh"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model//heads\n",
        "        self.h = heads\n",
        "        self.attn = None\n",
        "\n",
        "        # tạo ra 3 ma trận trọng số là q_linear, k_linear, v_linear như hình trên\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        q: batch_size x seq_length x d_model\n",
        "        k: batch_size x seq_length x d_model\n",
        "        v: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        bs = q.size(0)\n",
        "        # nhân ma trận trọng số q_linear, k_linear, v_linear với dữ liệu đầu vào q, k, v \n",
        "        # ở bước encode các bạn lưu ý rằng q, k, v chỉ là một (xem hình trên)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        \n",
        "        # tính attention score\n",
        "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
        "        \n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "        return output\n",
        "\n",
        "# MultiHeadAttention(8, 512)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 30, 512)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-_9Hq-NQIk"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        \n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        \n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ndbdMXNQIn"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Trong kiến trúc của chúng ta có tầng linear \n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "    \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wwo91xDNQIq"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        x2 = self.norm_1(x)\n",
        "        # tính attention value, các bạn để ý q, k, v là giống nhau        \n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# EncoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32 , 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mDt2NPeNQIu"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        \"\"\"\n",
        "        # Các bạn xem hình trên, kiến trúc mình vẽ với code ở chỗ này tương đương nhau.\n",
        "        x2 = self.norm_1(x)\n",
        "        # multihead attention thứ nhất, chú ý các từ ở target \n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        # masked mulithead attention thứ 2. k, v là giá trị output của mô hình encoder\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "# DecoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcU8nyvzNQIx"
      },
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Một encoder có nhiều encoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "        \n",
        "    def forward(self, src, mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Encoder(232, 512,6,8,0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lBRYMg_NQI0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Một decoder có nhiều decoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        trg: batch_size x seq_length\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpxSCRILNQI3"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        trg: batch_size x seq_length\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x vocab_size\n",
        "        \"\"\"\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        \n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "    \n",
        "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5tvzW9jNQI6"
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkBjLH96NQI8"
      },
      "source": [
        "\n",
        "def nopeak_mask(size, device):\n",
        "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
        "     mô hình không nhìn thấy được các từ ở tương lai\n",
        "    \"\"\"\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "    \n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, src_pad, trg_pad, device):\n",
        "    \"\"\" Tạo mask cho encoder, \n",
        "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào \n",
        "    \"\"\"\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, device)\n",
        "        if trg.is_cuda:\n",
        "            np_mask.cuda()\n",
        "        trg_mask = trg_mask & np_mask\n",
        "        \n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YoUVx4xjEb7"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "def get_synonym(word, SRC):\n",
        "    syns = wordnet.synsets(word)\n",
        "    for s in syns:\n",
        "        for l in s.lemmas():\n",
        "            if SRC.vocab.stoi[l.name()] != 0:\n",
        "                return SRC.vocab.stoi[l.name()]\n",
        "            \n",
        "    return 0\n",
        "\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IJpUEIMgMbw"
      },
      "source": [
        "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
        "    \"\"\"\n",
        "    init_tok = TRG.vocab.stoi['<sos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "\n",
        "    # tính sẵn output của encoder \n",
        "    e_output = model.encoder(src, src_mask)\n",
        "    \n",
        "    outputs = torch.LongTensor([[init_tok]])\n",
        "    \n",
        "    outputs = outputs.to(device)\n",
        "    \n",
        "    trg_mask = nopeak_mask(1, device)\n",
        "    # dự đoán kí tự đầu tiên\n",
        "    out = model.out(model.decoder(outputs,\n",
        "    e_output, src_mask, trg_mask))\n",
        "    out = F.softmax(out, dim=-1)\n",
        "    \n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "    \n",
        "    outputs = torch.zeros(k, max_len).long()\n",
        "    outputs = outputs.to(device)\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "    \n",
        "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
        "   \n",
        "    e_outputs = e_outputs.to(device)\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "    \n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "    \n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "    \n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "    \n",
        "    return outputs, log_scores\n",
        "\n",
        "def beam_search(src, model, SRC, TRG, device, k, max_len):    \n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
        "    eos_tok = TRG.vocab.stoi['<eos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "    ind = None\n",
        "    for i in range(2, max_len):\n",
        "    \n",
        "        trg_mask = nopeak_mask(i, device)\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i],\n",
        "        e_outputs, src_mask, trg_mask))\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "    \n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
        "        \n",
        "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
        "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == k:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "    \n",
        "    if ind is None:\n",
        "        \n",
        "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
        "    \n",
        "    else:\n",
        "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-AFuSOIhi7X"
      },
      "source": [
        "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\"Dịch một câu sử dụng beamsearch\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    indexed = []\n",
        "    sentence = SRC.preprocess(sentence)\n",
        "    \n",
        "    for tok in sentence:\n",
        "        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
        "            indexed.append(SRC.vocab.stoi[tok])\n",
        "        else:\n",
        "            indexed.append(get_synonym(tok, SRC))\n",
        "    \n",
        "    sentence = Variable(torch.LongTensor([indexed]))\n",
        "    \n",
        "    sentence = sentence.to(device)\n",
        "    \n",
        "    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
        "\n",
        "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uee4YaQNQI_"
      },
      "source": [
        "#tach cac ky tu va tao token\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "class tokenize(object):\n",
        "    \n",
        "    def __init__(self, lang):\n",
        "        self.nlp = spacy.load(lang)\n",
        "            \n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uVO0yr_NQJC"
      },
      "source": [
        "#load data\n",
        "import os\n",
        "import dill as pickle\n",
        "import pandas as pd\n",
        "\n",
        "def read_data(src_file, trg_file):\n",
        "    src_data = open(src_file).read().strip().split('\\n')\n",
        "\n",
        "    trg_data = open(trg_file).read().strip().split('\\n')\n",
        " \n",
        "    return src_data, trg_data\n",
        "\n",
        "def create_fields(src_lang, trg_lang):\n",
        "    \n",
        "    print(\"loading spacy tokenizers...\")\n",
        "    \n",
        "    t_src = tokenize(src_lang)\n",
        "    t_trg = tokenize(trg_lang)\n",
        "\n",
        "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
        "        \n",
        "    return SRC, TRG\n",
        "\n",
        "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
        "\n",
        "    print(\"creating dataset and iterator... \")\n",
        "\n",
        "    raw_data = {'src' : [line for line in src_data], 'trg': [line for line in trg_data]}\n",
        "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
        "    \n",
        "    mask = (df['src'].str.count(' ') < max_strlen) & (df['trg'].str.count(' ') < max_strlen)\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "    \n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    train_iter = MyIterator(train, batch_size=batchsize, device=device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=istrain, shuffle=True)\n",
        "    \n",
        "    os.remove('translate_transformer_temp.csv')\n",
        "    \n",
        "    if istrain:\n",
        "        SRC.build_vocab(train)\n",
        "        TRG.build_vocab(train)\n",
        "\n",
        "    return train_iter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4POJRxdNQJF"
      },
      "source": [
        "def step(model, optimizer,batch, criterion):\n",
        "    \"\"\"\n",
        "    Một lần cập nhật mô hình\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    src = batch.src.transpose(0,1).cuda()\n",
        "    trg = batch.trg.transpose(0,1).cuda()\n",
        "    trg_input = trg[:, :-1]\n",
        "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "    loss.backward()\n",
        "    optimizer.step_and_update_lr()\n",
        "    \n",
        "    loss = loss.item()\n",
        "    \n",
        "    return loss    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5sPA-k_NQJI"
      },
      "source": [
        "def validiate(model, valid_iter, criterion):\n",
        "    \"\"\" Tính loss trên tập validation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        for batch in valid_iter:\n",
        "            src = batch.src.transpose(0,1).cuda()\n",
        "            trg = batch.trg.transpose(0,1).cuda()\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "            \n",
        "            loss = loss.item()\n",
        "            \n",
        "            total_loss.append(loss)\n",
        "        \n",
        "    avg_loss = np.mean(total_loss)\n",
        "    \n",
        "    return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW8pRq91rwJR"
      },
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'init_lr':self.init_lr,\n",
        "            'd_model':self.d_model,\n",
        "            'n_warmup_steps':self.n_warmup_steps,\n",
        "            'n_steps':self.n_steps,\n",
        "            '_optimizer':self._optimizer.state_dict(),\n",
        "        }\n",
        "        \n",
        "        return optimizer_state_dict\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.init_lr = state_dict['init_lr']\n",
        "        self.d_model = state_dict['d_model']\n",
        "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
        "        self.n_steps = state_dict['n_steps']\n",
        "        \n",
        "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
        "        \n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHGeSHThtlj-"
      },
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0\n",
        "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "            if mask.dim() > 0:\n",
        "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "            \n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg257Gk_Kzzw"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen):\n",
        "    pred_sents = []\n",
        "    for sentence in valid_src_data:\n",
        "        pred_trg = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen)\n",
        "        pred_sents.append(pred_trg)\n",
        "    \n",
        "    pred_sents = [TRG.preprocess(sent) for sent in pred_sents]\n",
        "    trg_sents = [[sent.split()] for sent in valid_trg_data]\n",
        "    \n",
        "    return bleu_score(pred_sents, trg_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhgu-SPTNQJL"
      },
      "source": [
        "opt = {\n",
        "    'train_src_data':'train.en',\n",
        "    'train_trg_data':'train.vi',\n",
        "    'valid_src_data':'tst2013.en',\n",
        "    'valid_trg_data':'tst2013.vi',\n",
        "    'src_lang':'en',\n",
        "    'trg_lang':'en',#'vi_spacy_model',\n",
        "    'max_strlen':160,\n",
        "    'batchsize':1500,\n",
        "    'device':'cuda',\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'lr':0.0001,\n",
        "    'epochs':30,\n",
        "    'printevery': 200,\n",
        "    'k':5,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBotIB8pNQJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e215e811-725c-41d2-df78-63c7ed3b04e3"
      },
      "source": [
        "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
        "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
        "\n",
        "SRC, TRG = create_fields(opt['src_lang'], opt['trg_lang'])\n",
        "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
        "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading spacy tokenizers...\n",
            "creating dataset and iterator... \n",
            "creating dataset and iterator... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnw9xrJeNQJX"
      },
      "source": [
        "src_pad = SRC.vocab.stoi['<pad>']\n",
        "trg_pad = TRG.vocab.stoi['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RccNL8VNQJd"
      },
      "source": [
        "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(opt['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12debLGiNQJg"
      },
      "source": [
        "\n",
        "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),0.2, opt['d_model'], 4000)\n",
        "\n",
        "criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeZqfQPANQJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45038ace-58b4-42ec-c853-cab236df97b0"
      },
      "source": [
        "import time\n",
        "\n",
        "for epoch in range(opt['epochs']):\n",
        "    total_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(train_iter): \n",
        "        s = time.time()\n",
        "        loss = step(model, optimizer, batch, criterion)\n",
        "        \n",
        "        total_loss += loss\n",
        "        \n",
        "        if (i + 1) % opt['printevery'] == 0:\n",
        "            avg_loss = total_loss/opt['printevery']\n",
        "            print('epoch: {:03d} - iter: {:05d} - train loss: {:.4f} - time: {:.4f}'.format(epoch, i, avg_loss, time.time()- s))\n",
        "            total_loss = 0\n",
        "\n",
        "    s = time.time()\n",
        "    valid_loss = validiate(model, valid_iter, criterion)\n",
        "    bleuscore = bleu(valid_src_data[:500], valid_trg_data[:500], model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "    print('epoch: {:03d} - iter: {:05d} - valid loss: {:.4f} - bleu score: {:.4f} - time: {:.4f}'.format(epoch, i, valid_loss, bleuscore, time.time() - s))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 000 - iter: 00199 - train loss: 9.3618 - time: 0.2828\n",
            "epoch: 000 - iter: 00399 - train loss: 8.2459 - time: 0.2781\n",
            "epoch: 000 - iter: 00599 - train loss: 7.1711 - time: 0.2803\n",
            "epoch: 000 - iter: 00799 - train loss: 6.5171 - time: 0.2737\n",
            "epoch: 000 - iter: 00999 - train loss: 6.4509 - time: 0.2733\n",
            "epoch: 000 - iter: 01199 - train loss: 6.2684 - time: 0.2774\n",
            "epoch: 000 - iter: 01399 - train loss: 6.0909 - time: 0.2768\n",
            "epoch: 000 - iter: 01599 - train loss: 5.8973 - time: 0.2806\n",
            "epoch: 000 - iter: 01799 - train loss: 5.7150 - time: 0.2857\n",
            "epoch: 000 - iter: 01999 - train loss: 5.6359 - time: 0.2799\n",
            "epoch: 000 - iter: 02199 - train loss: 5.4610 - time: 0.3082\n",
            "epoch: 000 - iter: 02399 - train loss: 5.4328 - time: 0.2918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 000 - iter: 02497 - valid loss: 3.9604 - bleu score: 0.0155 - time: 193.2410\n",
            "epoch: 001 - iter: 00199 - train loss: 5.1327 - time: 0.2816\n",
            "epoch: 001 - iter: 00399 - train loss: 5.1282 - time: 0.2664\n",
            "epoch: 001 - iter: 00599 - train loss: 5.0423 - time: 0.2776\n",
            "epoch: 001 - iter: 00799 - train loss: 4.9621 - time: 0.3075\n",
            "epoch: 001 - iter: 00999 - train loss: 4.8081 - time: 0.2887\n",
            "epoch: 001 - iter: 01199 - train loss: 4.7674 - time: 0.2977\n",
            "epoch: 001 - iter: 01399 - train loss: 4.6795 - time: 0.2797\n",
            "epoch: 001 - iter: 01599 - train loss: 4.6669 - time: 0.2705\n",
            "epoch: 001 - iter: 01799 - train loss: 4.5664 - time: 0.2905\n",
            "epoch: 001 - iter: 01999 - train loss: 4.4334 - time: 0.2890\n",
            "epoch: 001 - iter: 02199 - train loss: 4.4122 - time: 0.3041\n",
            "epoch: 001 - iter: 02399 - train loss: 4.3436 - time: 0.2845\n",
            "epoch: 001 - iter: 02497 - valid loss: 3.1889 - bleu score: 0.1158 - time: 237.5987\n",
            "epoch: 002 - iter: 00199 - train loss: 4.2296 - time: 0.2670\n",
            "epoch: 002 - iter: 00399 - train loss: 4.1515 - time: 0.2862\n",
            "epoch: 002 - iter: 00599 - train loss: 4.0784 - time: 0.3067\n",
            "epoch: 002 - iter: 00799 - train loss: 4.0120 - time: 0.2845\n",
            "epoch: 002 - iter: 00999 - train loss: 3.9997 - time: 0.2375\n",
            "epoch: 002 - iter: 01199 - train loss: 3.9525 - time: 0.2886\n",
            "epoch: 002 - iter: 01399 - train loss: 3.9580 - time: 0.2774\n",
            "epoch: 002 - iter: 01599 - train loss: 3.9076 - time: 0.3131\n",
            "epoch: 002 - iter: 01799 - train loss: 3.8448 - time: 0.3021\n",
            "epoch: 002 - iter: 01999 - train loss: 3.8380 - time: 0.3048\n",
            "epoch: 002 - iter: 02199 - train loss: 3.7832 - time: 0.3025\n",
            "epoch: 002 - iter: 02399 - train loss: 3.7813 - time: 0.2989\n",
            "epoch: 002 - iter: 02497 - valid loss: 2.7750 - bleu score: 0.1810 - time: 230.0297\n",
            "epoch: 003 - iter: 00199 - train loss: 3.6868 - time: 0.2534\n",
            "epoch: 003 - iter: 00399 - train loss: 3.6251 - time: 0.3046\n",
            "epoch: 003 - iter: 00599 - train loss: 3.6242 - time: 0.2776\n",
            "epoch: 003 - iter: 00799 - train loss: 3.5743 - time: 0.2663\n",
            "epoch: 003 - iter: 00999 - train loss: 3.5566 - time: 0.3031\n",
            "epoch: 003 - iter: 01199 - train loss: 3.5638 - time: 0.3048\n",
            "epoch: 003 - iter: 01399 - train loss: 3.5482 - time: 0.2759\n",
            "epoch: 003 - iter: 01599 - train loss: 3.5263 - time: 0.2730\n",
            "epoch: 003 - iter: 01799 - train loss: 3.5760 - time: 0.2690\n",
            "epoch: 003 - iter: 01999 - train loss: 3.5194 - time: 0.2865\n",
            "epoch: 003 - iter: 02199 - train loss: 3.5195 - time: 0.2900\n",
            "epoch: 003 - iter: 02399 - train loss: 3.4634 - time: 0.2675\n",
            "epoch: 003 - iter: 02497 - valid loss: 2.6188 - bleu score: 0.2089 - time: 223.7998\n",
            "epoch: 004 - iter: 00199 - train loss: 3.3340 - time: 0.2628\n",
            "epoch: 004 - iter: 00399 - train loss: 3.3742 - time: 0.3000\n",
            "epoch: 004 - iter: 00599 - train loss: 3.3999 - time: 0.2810\n",
            "epoch: 004 - iter: 00799 - train loss: 3.3490 - time: 0.2758\n",
            "epoch: 004 - iter: 00999 - train loss: 3.3599 - time: 0.2789\n",
            "epoch: 004 - iter: 01199 - train loss: 3.3472 - time: 0.2547\n",
            "epoch: 004 - iter: 01399 - train loss: 3.3137 - time: 0.2591\n",
            "epoch: 004 - iter: 01599 - train loss: 3.3669 - time: 0.2968\n",
            "epoch: 004 - iter: 01799 - train loss: 3.3212 - time: 0.2814\n",
            "epoch: 004 - iter: 01999 - train loss: 3.3496 - time: 0.2662\n",
            "epoch: 004 - iter: 02199 - train loss: 3.2794 - time: 0.2990\n",
            "epoch: 004 - iter: 02399 - train loss: 3.3506 - time: 0.2683\n",
            "epoch: 004 - iter: 02497 - valid loss: 2.5180 - bleu score: 0.2291 - time: 235.7255\n",
            "epoch: 005 - iter: 00199 - train loss: 3.1884 - time: 0.2743\n",
            "epoch: 005 - iter: 00399 - train loss: 3.2000 - time: 0.2917\n",
            "epoch: 005 - iter: 00599 - train loss: 3.2166 - time: 0.2794\n",
            "epoch: 005 - iter: 00799 - train loss: 3.1905 - time: 0.2687\n",
            "epoch: 005 - iter: 00999 - train loss: 3.2446 - time: 0.2980\n",
            "epoch: 005 - iter: 01199 - train loss: 3.2443 - time: 0.2850\n",
            "epoch: 005 - iter: 01399 - train loss: 3.1609 - time: 0.2743\n",
            "epoch: 005 - iter: 01599 - train loss: 3.1918 - time: 0.3027\n",
            "epoch: 005 - iter: 01799 - train loss: 3.1862 - time: 0.2804\n",
            "epoch: 005 - iter: 01999 - train loss: 3.1985 - time: 0.3012\n",
            "epoch: 005 - iter: 02199 - train loss: 3.2011 - time: 0.2586\n",
            "epoch: 005 - iter: 02399 - train loss: 3.1848 - time: 0.2814\n",
            "epoch: 005 - iter: 02497 - valid loss: 2.4619 - bleu score: 0.2404 - time: 231.3349\n",
            "epoch: 006 - iter: 00199 - train loss: 3.0516 - time: 0.2653\n",
            "epoch: 006 - iter: 00399 - train loss: 3.1029 - time: 0.2845\n",
            "epoch: 006 - iter: 00599 - train loss: 3.0778 - time: 0.2690\n",
            "epoch: 006 - iter: 00799 - train loss: 3.1118 - time: 0.2700\n",
            "epoch: 006 - iter: 00999 - train loss: 3.1010 - time: 0.2827\n",
            "epoch: 006 - iter: 01199 - train loss: 3.1103 - time: 0.2667\n",
            "epoch: 006 - iter: 01399 - train loss: 3.0896 - time: 0.2814\n",
            "epoch: 006 - iter: 01599 - train loss: 3.1433 - time: 0.2799\n",
            "epoch: 006 - iter: 01799 - train loss: 3.0930 - time: 0.2831\n",
            "epoch: 006 - iter: 01999 - train loss: 3.1045 - time: 0.3004\n",
            "epoch: 006 - iter: 02199 - train loss: 3.1148 - time: 0.2865\n",
            "epoch: 006 - iter: 02399 - train loss: 3.0816 - time: 0.2545\n",
            "epoch: 006 - iter: 02497 - valid loss: 2.4299 - bleu score: 0.2463 - time: 228.1851\n",
            "epoch: 007 - iter: 00199 - train loss: 2.9799 - time: 0.2769\n",
            "epoch: 007 - iter: 00399 - train loss: 2.9845 - time: 0.2773\n",
            "epoch: 007 - iter: 00599 - train loss: 2.9956 - time: 0.2893\n",
            "epoch: 007 - iter: 00799 - train loss: 3.0433 - time: 0.2769\n",
            "epoch: 007 - iter: 00999 - train loss: 3.0224 - time: 0.2819\n",
            "epoch: 007 - iter: 01199 - train loss: 3.0216 - time: 0.2769\n",
            "epoch: 007 - iter: 01399 - train loss: 3.0129 - time: 0.2863\n",
            "epoch: 007 - iter: 01599 - train loss: 3.0082 - time: 0.2966\n",
            "epoch: 007 - iter: 01799 - train loss: 3.0509 - time: 0.2672\n",
            "epoch: 007 - iter: 01999 - train loss: 3.0053 - time: 0.2815\n",
            "epoch: 007 - iter: 02199 - train loss: 3.0485 - time: 0.2720\n",
            "epoch: 007 - iter: 02399 - train loss: 2.9982 - time: 0.3078\n",
            "epoch: 007 - iter: 02497 - valid loss: 2.4058 - bleu score: 0.2449 - time: 230.6837\n",
            "epoch: 008 - iter: 00199 - train loss: 2.9391 - time: 0.2627\n",
            "epoch: 008 - iter: 00399 - train loss: 2.9562 - time: 0.2755\n",
            "epoch: 008 - iter: 00599 - train loss: 2.9457 - time: 0.2610\n",
            "epoch: 008 - iter: 00799 - train loss: 2.9893 - time: 0.2789\n",
            "epoch: 008 - iter: 00999 - train loss: 2.9108 - time: 0.2811\n",
            "epoch: 008 - iter: 01199 - train loss: 2.9491 - time: 0.2816\n",
            "epoch: 008 - iter: 01399 - train loss: 2.9577 - time: 0.2674\n",
            "epoch: 008 - iter: 01599 - train loss: 2.9427 - time: 0.2886\n",
            "epoch: 008 - iter: 01799 - train loss: 2.9464 - time: 0.2678\n",
            "epoch: 008 - iter: 01999 - train loss: 2.9369 - time: 0.3017\n",
            "epoch: 008 - iter: 02199 - train loss: 2.9565 - time: 0.2853\n",
            "epoch: 008 - iter: 02399 - train loss: 2.9482 - time: 0.2606\n",
            "epoch: 008 - iter: 02497 - valid loss: 2.3901 - bleu score: 0.2510 - time: 234.9110\n",
            "epoch: 009 - iter: 00199 - train loss: 2.8955 - time: 0.2642\n",
            "epoch: 009 - iter: 00399 - train loss: 2.8828 - time: 0.2871\n",
            "epoch: 009 - iter: 00599 - train loss: 2.8723 - time: 0.2781\n",
            "epoch: 009 - iter: 00799 - train loss: 2.8905 - time: 0.2791\n",
            "epoch: 009 - iter: 00999 - train loss: 2.8758 - time: 0.2811\n",
            "epoch: 009 - iter: 01199 - train loss: 2.8827 - time: 0.3021\n",
            "epoch: 009 - iter: 01399 - train loss: 2.8954 - time: 0.2730\n",
            "epoch: 009 - iter: 01599 - train loss: 2.8959 - time: 0.2750\n",
            "epoch: 009 - iter: 01799 - train loss: 2.8882 - time: 0.2958\n",
            "epoch: 009 - iter: 01999 - train loss: 2.8984 - time: 0.2491\n",
            "epoch: 009 - iter: 02199 - train loss: 2.8911 - time: 0.2793\n",
            "epoch: 009 - iter: 02399 - train loss: 2.9135 - time: 0.2840\n",
            "epoch: 009 - iter: 02497 - valid loss: 2.3764 - bleu score: 0.2523 - time: 234.4570\n",
            "epoch: 010 - iter: 00199 - train loss: 2.8186 - time: 0.2717\n",
            "epoch: 010 - iter: 00399 - train loss: 2.8431 - time: 0.3001\n",
            "epoch: 010 - iter: 00599 - train loss: 2.8337 - time: 0.2835\n",
            "epoch: 010 - iter: 00799 - train loss: 2.8130 - time: 0.3020\n",
            "epoch: 010 - iter: 00999 - train loss: 2.8450 - time: 0.3052\n",
            "epoch: 010 - iter: 01199 - train loss: 2.8027 - time: 0.2672\n",
            "epoch: 010 - iter: 01399 - train loss: 2.8855 - time: 0.2823\n",
            "epoch: 010 - iter: 01599 - train loss: 2.8682 - time: 0.2908\n",
            "epoch: 010 - iter: 01799 - train loss: 2.8498 - time: 0.2785\n",
            "epoch: 010 - iter: 01999 - train loss: 2.8414 - time: 0.3091\n",
            "epoch: 010 - iter: 02199 - train loss: 2.8510 - time: 0.3056\n",
            "epoch: 010 - iter: 02399 - train loss: 2.8251 - time: 0.3008\n",
            "epoch: 010 - iter: 02497 - valid loss: 2.3711 - bleu score: 0.2494 - time: 234.6890\n",
            "epoch: 011 - iter: 00199 - train loss: 2.7875 - time: 0.2689\n",
            "epoch: 011 - iter: 00399 - train loss: 2.7722 - time: 0.3018\n",
            "epoch: 011 - iter: 00599 - train loss: 2.8148 - time: 0.2894\n",
            "epoch: 011 - iter: 00799 - train loss: 2.7401 - time: 0.2992\n",
            "epoch: 011 - iter: 00999 - train loss: 2.8027 - time: 0.2864\n",
            "epoch: 011 - iter: 01199 - train loss: 2.7897 - time: 0.2769\n",
            "epoch: 011 - iter: 01399 - train loss: 2.8053 - time: 0.2979\n",
            "epoch: 011 - iter: 01599 - train loss: 2.8059 - time: 0.2714\n",
            "epoch: 011 - iter: 01799 - train loss: 2.8257 - time: 0.2790\n",
            "epoch: 011 - iter: 01999 - train loss: 2.8048 - time: 0.2853\n",
            "epoch: 011 - iter: 02199 - train loss: 2.8253 - time: 0.2747\n",
            "epoch: 011 - iter: 02399 - train loss: 2.8026 - time: 0.2840\n",
            "epoch: 011 - iter: 02497 - valid loss: 2.3647 - bleu score: 0.2548 - time: 234.3878\n",
            "epoch: 012 - iter: 00199 - train loss: 2.7643 - time: 0.2871\n",
            "epoch: 012 - iter: 00399 - train loss: 2.7422 - time: 0.3014\n",
            "epoch: 012 - iter: 00599 - train loss: 2.7536 - time: 0.2570\n",
            "epoch: 012 - iter: 00799 - train loss: 2.7407 - time: 0.2828\n",
            "epoch: 012 - iter: 00999 - train loss: 2.7435 - time: 0.2866\n",
            "epoch: 012 - iter: 01199 - train loss: 2.7291 - time: 0.2755\n",
            "epoch: 012 - iter: 01399 - train loss: 2.7873 - time: 0.2656\n",
            "epoch: 012 - iter: 01599 - train loss: 2.7413 - time: 0.2708\n",
            "epoch: 012 - iter: 01799 - train loss: 2.7767 - time: 0.2617\n",
            "epoch: 012 - iter: 01999 - train loss: 2.7924 - time: 0.2881\n",
            "epoch: 012 - iter: 02199 - train loss: 2.7438 - time: 0.2810\n",
            "epoch: 012 - iter: 02399 - train loss: 2.7803 - time: 0.3087\n",
            "epoch: 012 - iter: 02497 - valid loss: 2.3557 - bleu score: 0.2552 - time: 241.8331\n",
            "epoch: 013 - iter: 00199 - train loss: 2.6805 - time: 0.2811\n",
            "epoch: 013 - iter: 00399 - train loss: 2.6984 - time: 0.2736\n",
            "epoch: 013 - iter: 00599 - train loss: 2.7099 - time: 0.2871\n",
            "epoch: 013 - iter: 00799 - train loss: 2.7090 - time: 0.2646\n",
            "epoch: 013 - iter: 00999 - train loss: 2.7367 - time: 0.2685\n",
            "epoch: 013 - iter: 01199 - train loss: 2.7387 - time: 0.3009\n",
            "epoch: 013 - iter: 01399 - train loss: 2.6940 - time: 0.2924\n",
            "epoch: 013 - iter: 01599 - train loss: 2.7358 - time: 0.2704\n",
            "epoch: 013 - iter: 01799 - train loss: 2.7329 - time: 0.2885\n",
            "epoch: 013 - iter: 01999 - train loss: 2.7383 - time: 0.3099\n",
            "epoch: 013 - iter: 02199 - train loss: 2.7209 - time: 0.2768\n",
            "epoch: 013 - iter: 02399 - train loss: 2.7539 - time: 0.2871\n",
            "epoch: 013 - iter: 02497 - valid loss: 2.3573 - bleu score: 0.2559 - time: 239.7389\n",
            "epoch: 014 - iter: 00199 - train loss: 2.6551 - time: 0.2661\n",
            "epoch: 014 - iter: 00399 - train loss: 2.6840 - time: 0.2787\n",
            "epoch: 014 - iter: 00599 - train loss: 2.6925 - time: 0.2802\n",
            "epoch: 014 - iter: 00799 - train loss: 2.6877 - time: 0.2813\n",
            "epoch: 014 - iter: 00999 - train loss: 2.7094 - time: 0.2803\n",
            "epoch: 014 - iter: 01199 - train loss: 2.6840 - time: 0.2814\n",
            "epoch: 014 - iter: 01399 - train loss: 2.6999 - time: 0.3098\n",
            "epoch: 014 - iter: 01599 - train loss: 2.6897 - time: 0.2691\n",
            "epoch: 014 - iter: 01799 - train loss: 2.6768 - time: 0.2997\n",
            "epoch: 014 - iter: 01999 - train loss: 2.7326 - time: 0.2800\n",
            "epoch: 014 - iter: 02199 - train loss: 2.6794 - time: 0.2854\n",
            "epoch: 014 - iter: 02399 - train loss: 2.6891 - time: 0.2707\n",
            "epoch: 014 - iter: 02497 - valid loss: 2.3579 - bleu score: 0.2575 - time: 232.9282\n",
            "epoch: 015 - iter: 00199 - train loss: 2.6438 - time: 0.2774\n",
            "epoch: 015 - iter: 00399 - train loss: 2.6395 - time: 0.2959\n",
            "epoch: 015 - iter: 00599 - train loss: 2.6360 - time: 0.2883\n",
            "epoch: 015 - iter: 00799 - train loss: 2.6414 - time: 0.2763\n",
            "epoch: 015 - iter: 00999 - train loss: 2.6754 - time: 0.2791\n",
            "epoch: 015 - iter: 01199 - train loss: 2.6507 - time: 0.2851\n",
            "epoch: 015 - iter: 01399 - train loss: 2.6792 - time: 0.3016\n",
            "epoch: 015 - iter: 01599 - train loss: 2.6642 - time: 0.3094\n",
            "epoch: 015 - iter: 01799 - train loss: 2.6672 - time: 0.2936\n",
            "epoch: 015 - iter: 01999 - train loss: 2.6883 - time: 0.2821\n",
            "epoch: 015 - iter: 02199 - train loss: 2.6304 - time: 0.2781\n",
            "epoch: 015 - iter: 02399 - train loss: 2.6820 - time: 0.2801\n",
            "epoch: 015 - iter: 02497 - valid loss: 2.3556 - bleu score: 0.2569 - time: 243.5352\n",
            "epoch: 016 - iter: 00199 - train loss: 2.6236 - time: 0.2765\n",
            "epoch: 016 - iter: 00399 - train loss: 2.6087 - time: 0.2780\n",
            "epoch: 016 - iter: 00599 - train loss: 2.6268 - time: 0.3080\n",
            "epoch: 016 - iter: 00799 - train loss: 2.6186 - time: 0.2601\n",
            "epoch: 016 - iter: 00999 - train loss: 2.6528 - time: 0.2873\n",
            "epoch: 016 - iter: 01199 - train loss: 2.6543 - time: 0.2670\n",
            "epoch: 016 - iter: 01399 - train loss: 2.6367 - time: 0.3080\n",
            "epoch: 016 - iter: 01599 - train loss: 2.6013 - time: 0.2842\n",
            "epoch: 016 - iter: 01799 - train loss: 2.6510 - time: 0.2835\n",
            "epoch: 016 - iter: 01999 - train loss: 2.6344 - time: 0.2748\n",
            "epoch: 016 - iter: 02199 - train loss: 2.6135 - time: 0.2765\n",
            "epoch: 016 - iter: 02399 - train loss: 2.6411 - time: 0.2932\n",
            "epoch: 016 - iter: 02497 - valid loss: 2.3540 - bleu score: 0.2606 - time: 242.3912\n",
            "epoch: 017 - iter: 00199 - train loss: 2.5723 - time: 0.2834\n",
            "epoch: 017 - iter: 00399 - train loss: 2.5604 - time: 0.2828\n",
            "epoch: 017 - iter: 00599 - train loss: 2.5970 - time: 0.2719\n",
            "epoch: 017 - iter: 00799 - train loss: 2.6170 - time: 0.2859\n",
            "epoch: 017 - iter: 00999 - train loss: 2.5971 - time: 0.2656\n",
            "epoch: 017 - iter: 01199 - train loss: 2.6020 - time: 0.2844\n",
            "epoch: 017 - iter: 01399 - train loss: 2.6342 - time: 0.3004\n",
            "epoch: 017 - iter: 01599 - train loss: 2.5958 - time: 0.2761\n",
            "epoch: 017 - iter: 01799 - train loss: 2.6116 - time: 0.2614\n",
            "epoch: 017 - iter: 01999 - train loss: 2.6155 - time: 0.2837\n",
            "epoch: 017 - iter: 02199 - train loss: 2.6212 - time: 0.2671\n",
            "epoch: 017 - iter: 02399 - train loss: 2.6124 - time: 0.3101\n",
            "epoch: 017 - iter: 02497 - valid loss: 2.3515 - bleu score: 0.2617 - time: 235.9493\n",
            "epoch: 018 - iter: 00199 - train loss: 2.5158 - time: 0.2771\n",
            "epoch: 018 - iter: 00399 - train loss: 2.5581 - time: 0.2408\n",
            "epoch: 018 - iter: 00599 - train loss: 2.5620 - time: 0.3032\n",
            "epoch: 018 - iter: 00799 - train loss: 2.5948 - time: 0.2907\n",
            "epoch: 018 - iter: 00999 - train loss: 2.5563 - time: 0.2828\n",
            "epoch: 018 - iter: 01199 - train loss: 2.5770 - time: 0.2600\n",
            "epoch: 018 - iter: 01399 - train loss: 2.5844 - time: 0.2803\n",
            "epoch: 018 - iter: 01599 - train loss: 2.5817 - time: 0.3061\n",
            "epoch: 018 - iter: 01799 - train loss: 2.5834 - time: 0.2860\n",
            "epoch: 018 - iter: 01999 - train loss: 2.6045 - time: 0.2693\n",
            "epoch: 018 - iter: 02199 - train loss: 2.6132 - time: 0.2828\n",
            "epoch: 018 - iter: 02399 - train loss: 2.6166 - time: 0.2798\n",
            "epoch: 018 - iter: 02497 - valid loss: 2.3596 - bleu score: 0.2619 - time: 235.6431\n",
            "epoch: 019 - iter: 00199 - train loss: 2.5174 - time: 0.2743\n",
            "epoch: 019 - iter: 00399 - train loss: 2.5584 - time: 0.3106\n",
            "epoch: 019 - iter: 00599 - train loss: 2.5615 - time: 0.3049\n",
            "epoch: 019 - iter: 00799 - train loss: 2.5416 - time: 0.2698\n",
            "epoch: 019 - iter: 00999 - train loss: 2.5143 - time: 0.2710\n",
            "epoch: 019 - iter: 01199 - train loss: 2.5565 - time: 0.2841\n",
            "epoch: 019 - iter: 01399 - train loss: 2.5670 - time: 0.3076\n",
            "epoch: 019 - iter: 01599 - train loss: 2.5659 - time: 0.2990\n",
            "epoch: 019 - iter: 01799 - train loss: 2.5570 - time: 0.2701\n",
            "epoch: 019 - iter: 01999 - train loss: 2.5738 - time: 0.3067\n",
            "epoch: 019 - iter: 02199 - train loss: 2.5614 - time: 0.2838\n",
            "epoch: 019 - iter: 02399 - train loss: 2.5586 - time: 0.3043\n",
            "epoch: 019 - iter: 02497 - valid loss: 2.3682 - bleu score: 0.2596 - time: 239.5321\n",
            "epoch: 020 - iter: 00199 - train loss: 2.5293 - time: 0.2897\n",
            "epoch: 020 - iter: 00399 - train loss: 2.5227 - time: 0.2833\n",
            "epoch: 020 - iter: 00599 - train loss: 2.5308 - time: 0.2979\n",
            "epoch: 020 - iter: 00799 - train loss: 2.5297 - time: 0.2548\n",
            "epoch: 020 - iter: 00999 - train loss: 2.5214 - time: 0.2853\n",
            "epoch: 020 - iter: 01199 - train loss: 2.5232 - time: 0.2761\n",
            "epoch: 020 - iter: 01399 - train loss: 2.5301 - time: 0.2805\n",
            "epoch: 020 - iter: 01599 - train loss: 2.5210 - time: 0.2610\n",
            "epoch: 020 - iter: 01799 - train loss: 2.5254 - time: 0.2998\n",
            "epoch: 020 - iter: 01999 - train loss: 2.5538 - time: 0.2932\n",
            "epoch: 020 - iter: 02199 - train loss: 2.5459 - time: 0.2769\n",
            "epoch: 020 - iter: 02399 - train loss: 2.5417 - time: 0.2798\n",
            "epoch: 020 - iter: 02497 - valid loss: 2.3686 - bleu score: 0.2591 - time: 236.3471\n",
            "epoch: 021 - iter: 00199 - train loss: 2.4803 - time: 0.2717\n",
            "epoch: 021 - iter: 00399 - train loss: 2.5030 - time: 0.2712\n",
            "epoch: 021 - iter: 00599 - train loss: 2.4998 - time: 0.2081\n",
            "epoch: 021 - iter: 00799 - train loss: 2.5154 - time: 0.3007\n",
            "epoch: 021 - iter: 00999 - train loss: 2.5250 - time: 0.2736\n",
            "epoch: 021 - iter: 01199 - train loss: 2.4760 - time: 0.2827\n",
            "epoch: 021 - iter: 01399 - train loss: 2.5235 - time: 0.3033\n",
            "epoch: 021 - iter: 01599 - train loss: 2.5142 - time: 0.2687\n",
            "epoch: 021 - iter: 01799 - train loss: 2.5077 - time: 0.2787\n",
            "epoch: 021 - iter: 01999 - train loss: 2.5252 - time: 0.2650\n",
            "epoch: 021 - iter: 02199 - train loss: 2.5159 - time: 0.2691\n",
            "epoch: 021 - iter: 02399 - train loss: 2.5317 - time: 0.3030\n",
            "epoch: 021 - iter: 02497 - valid loss: 2.3668 - bleu score: 0.2573 - time: 241.1891\n",
            "epoch: 022 - iter: 00199 - train loss: 2.4547 - time: 0.2712\n",
            "epoch: 022 - iter: 00399 - train loss: 2.4720 - time: 0.2990\n",
            "epoch: 022 - iter: 00599 - train loss: 2.4773 - time: 0.2814\n",
            "epoch: 022 - iter: 00799 - train loss: 2.4681 - time: 0.2778\n",
            "epoch: 022 - iter: 00999 - train loss: 2.4704 - time: 0.3039\n",
            "epoch: 022 - iter: 01199 - train loss: 2.4797 - time: 0.2827\n",
            "epoch: 022 - iter: 01399 - train loss: 2.4778 - time: 0.2843\n",
            "epoch: 022 - iter: 01599 - train loss: 2.5170 - time: 0.2841\n",
            "epoch: 022 - iter: 01799 - train loss: 2.4951 - time: 0.2898\n",
            "epoch: 022 - iter: 01999 - train loss: 2.5289 - time: 0.2815\n",
            "epoch: 022 - iter: 02199 - train loss: 2.5000 - time: 0.2833\n",
            "epoch: 022 - iter: 02399 - train loss: 2.5267 - time: 0.2841\n",
            "epoch: 022 - iter: 02497 - valid loss: 2.3759 - bleu score: 0.2596 - time: 244.5128\n",
            "epoch: 023 - iter: 00199 - train loss: 2.4558 - time: 0.3023\n",
            "epoch: 023 - iter: 00399 - train loss: 2.4607 - time: 0.2826\n",
            "epoch: 023 - iter: 00599 - train loss: 2.4370 - time: 0.2900\n",
            "epoch: 023 - iter: 00799 - train loss: 2.4414 - time: 0.3038\n",
            "epoch: 023 - iter: 00999 - train loss: 2.4731 - time: 0.2642\n",
            "epoch: 023 - iter: 01199 - train loss: 2.4759 - time: 0.3057\n",
            "epoch: 023 - iter: 01399 - train loss: 2.4767 - time: 0.2704\n",
            "epoch: 023 - iter: 01599 - train loss: 2.4606 - time: 0.3053\n",
            "epoch: 023 - iter: 01799 - train loss: 2.4852 - time: 0.3024\n",
            "epoch: 023 - iter: 01999 - train loss: 2.4857 - time: 0.2910\n",
            "epoch: 023 - iter: 02199 - train loss: 2.4812 - time: 0.3019\n",
            "epoch: 023 - iter: 02399 - train loss: 2.5079 - time: 0.2903\n",
            "epoch: 023 - iter: 02497 - valid loss: 2.3834 - bleu score: 0.2569 - time: 239.1515\n",
            "epoch: 024 - iter: 00199 - train loss: 2.4309 - time: 0.2742\n",
            "epoch: 024 - iter: 00399 - train loss: 2.4508 - time: 0.2809\n",
            "epoch: 024 - iter: 00599 - train loss: 2.4516 - time: 0.2625\n",
            "epoch: 024 - iter: 00799 - train loss: 2.4331 - time: 0.2825\n",
            "epoch: 024 - iter: 00999 - train loss: 2.4763 - time: 0.2798\n",
            "epoch: 024 - iter: 01199 - train loss: 2.4251 - time: 0.2976\n",
            "epoch: 024 - iter: 01399 - train loss: 2.4453 - time: 0.2886\n",
            "epoch: 024 - iter: 01599 - train loss: 2.4663 - time: 0.2812\n",
            "epoch: 024 - iter: 01799 - train loss: 2.4585 - time: 0.2516\n",
            "epoch: 024 - iter: 01999 - train loss: 2.4521 - time: 0.2822\n",
            "epoch: 024 - iter: 02199 - train loss: 2.4420 - time: 0.2812\n",
            "epoch: 024 - iter: 02399 - train loss: 2.4743 - time: 0.2809\n",
            "epoch: 024 - iter: 02497 - valid loss: 2.3804 - bleu score: 0.2625 - time: 238.2937\n",
            "epoch: 025 - iter: 00199 - train loss: 2.4269 - time: 0.2624\n",
            "epoch: 025 - iter: 00399 - train loss: 2.4192 - time: 0.2788\n",
            "epoch: 025 - iter: 00599 - train loss: 2.4268 - time: 0.3051\n",
            "epoch: 025 - iter: 00799 - train loss: 2.4109 - time: 0.3095\n",
            "epoch: 025 - iter: 00999 - train loss: 2.4223 - time: 0.2832\n",
            "epoch: 025 - iter: 01199 - train loss: 2.4346 - time: 0.2598\n",
            "epoch: 025 - iter: 01399 - train loss: 2.4530 - time: 0.2660\n",
            "epoch: 025 - iter: 01599 - train loss: 2.4601 - time: 0.2407\n",
            "epoch: 025 - iter: 01799 - train loss: 2.4644 - time: 0.2805\n",
            "epoch: 025 - iter: 01999 - train loss: 2.4394 - time: 0.3016\n",
            "epoch: 025 - iter: 02199 - train loss: 2.4234 - time: 0.2852\n",
            "epoch: 025 - iter: 02399 - train loss: 2.4298 - time: 0.2976\n",
            "epoch: 025 - iter: 02497 - valid loss: 2.3873 - bleu score: 0.2628 - time: 241.4872\n",
            "epoch: 026 - iter: 00199 - train loss: 2.3746 - time: 0.2775\n",
            "epoch: 026 - iter: 00399 - train loss: 2.4033 - time: 0.2826\n",
            "epoch: 026 - iter: 00599 - train loss: 2.4035 - time: 0.3040\n",
            "epoch: 026 - iter: 00799 - train loss: 2.4141 - time: 0.2974\n",
            "epoch: 026 - iter: 00999 - train loss: 2.3973 - time: 0.2836\n",
            "epoch: 026 - iter: 01199 - train loss: 2.4280 - time: 0.2732\n",
            "epoch: 026 - iter: 01399 - train loss: 2.4210 - time: 0.2755\n",
            "epoch: 026 - iter: 01599 - train loss: 2.4126 - time: 0.2788\n",
            "epoch: 026 - iter: 01799 - train loss: 2.4465 - time: 0.2841\n",
            "epoch: 026 - iter: 01999 - train loss: 2.4205 - time: 0.2828\n",
            "epoch: 026 - iter: 02199 - train loss: 2.4365 - time: 0.2975\n",
            "epoch: 026 - iter: 02399 - train loss: 2.4272 - time: 0.2683\n",
            "epoch: 026 - iter: 02497 - valid loss: 2.3888 - bleu score: 0.2595 - time: 237.4346\n",
            "epoch: 027 - iter: 00199 - train loss: 2.3838 - time: 0.2732\n",
            "epoch: 027 - iter: 00399 - train loss: 2.3942 - time: 0.2773\n",
            "epoch: 027 - iter: 00599 - train loss: 2.3945 - time: 0.3070\n",
            "epoch: 027 - iter: 00799 - train loss: 2.4195 - time: 0.2728\n",
            "epoch: 027 - iter: 00999 - train loss: 2.3976 - time: 0.2976\n",
            "epoch: 027 - iter: 01199 - train loss: 2.4205 - time: 0.3008\n",
            "epoch: 027 - iter: 01399 - train loss: 2.3776 - time: 0.2983\n",
            "epoch: 027 - iter: 01599 - train loss: 2.3891 - time: 0.2875\n",
            "epoch: 027 - iter: 01799 - train loss: 2.3792 - time: 0.2731\n",
            "epoch: 027 - iter: 01999 - train loss: 2.4134 - time: 0.2947\n",
            "epoch: 027 - iter: 02199 - train loss: 2.3941 - time: 0.3029\n",
            "epoch: 027 - iter: 02399 - train loss: 2.4158 - time: 0.3102\n",
            "epoch: 027 - iter: 02497 - valid loss: 2.3990 - bleu score: 0.2614 - time: 238.6579\n",
            "epoch: 028 - iter: 00199 - train loss: 2.3583 - time: 0.3044\n",
            "epoch: 028 - iter: 00399 - train loss: 2.3618 - time: 0.2841\n",
            "epoch: 028 - iter: 00599 - train loss: 2.3608 - time: 0.2835\n",
            "epoch: 028 - iter: 00799 - train loss: 2.3991 - time: 0.2969\n",
            "epoch: 028 - iter: 00999 - train loss: 2.3574 - time: 0.2747\n",
            "epoch: 028 - iter: 01199 - train loss: 2.3882 - time: 0.2967\n",
            "epoch: 028 - iter: 01399 - train loss: 2.3615 - time: 0.3001\n",
            "epoch: 028 - iter: 01599 - train loss: 2.3844 - time: 0.2933\n",
            "epoch: 028 - iter: 01799 - train loss: 2.3988 - time: 0.2871\n",
            "epoch: 028 - iter: 01999 - train loss: 2.3963 - time: 0.2838\n",
            "epoch: 028 - iter: 02199 - train loss: 2.4041 - time: 0.2808\n",
            "epoch: 028 - iter: 02399 - train loss: 2.4055 - time: 0.2766\n",
            "epoch: 028 - iter: 02497 - valid loss: 2.4023 - bleu score: 0.2636 - time: 238.9899\n",
            "epoch: 029 - iter: 00199 - train loss: 2.3589 - time: 0.2832\n",
            "epoch: 029 - iter: 00399 - train loss: 2.3703 - time: 0.2883\n",
            "epoch: 029 - iter: 00599 - train loss: 2.3438 - time: 0.3057\n",
            "epoch: 029 - iter: 00799 - train loss: 2.3417 - time: 0.3034\n",
            "epoch: 029 - iter: 00999 - train loss: 2.3572 - time: 0.2844\n",
            "epoch: 029 - iter: 01199 - train loss: 2.3710 - time: 0.2516\n",
            "epoch: 029 - iter: 01399 - train loss: 2.3656 - time: 0.2836\n",
            "epoch: 029 - iter: 01599 - train loss: 2.3464 - time: 0.2768\n",
            "epoch: 029 - iter: 01799 - train loss: 2.3871 - time: 0.2704\n",
            "epoch: 029 - iter: 01999 - train loss: 2.3818 - time: 0.3071\n",
            "epoch: 029 - iter: 02199 - train loss: 2.3892 - time: 0.3082\n",
            "epoch: 029 - iter: 02399 - train loss: 2.3865 - time: 0.3053\n",
            "epoch: 029 - iter: 02497 - valid loss: 2.4028 - bleu score: 0.2628 - time: 242.6533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK1On38Vgei_",
        "outputId": "7791c60f-0262-413d-8ddb-cd2cbd3df306"
      },
      "source": [
        "! gdown --id 1Ty1bGrd0sCwEqXhsoViCUaNKa3lFwmPH\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ty1bGrd0sCwEqXhsoViCUaNKa3lFwmPH\n",
            "To: /content/transformer.pth\n",
            "348MB [00:01, 178MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgXtTt8_i5Am",
        "outputId": "0d52c2d6-3763-41d5-8ba9-24f6fecbc601"
      },
      "source": [
        "model.load_state_dict(torch.load('./transformer.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZiEkF9lq3ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f545aaa-7ada-4931-802b-e46976c983ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDoIhGjRjrAW",
        "outputId": "f2bf0ac0-9f2b-47af-d1b8-a69974469577"
      },
      "source": [
        "bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2516075670719147"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lsVT3-RgHg3p",
        "outputId": "1878b351-fd48-4641-fb5b-5817ffe0a718"
      },
      "source": [
        "sentence='you must leave now .'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bạn phải bỏ đi ngay bây giờ.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VdZY8yrUIeDD",
        "outputId": "ce010a7a-ca14-4d7f-e2a5-23d166b707aa"
      },
      "source": [
        "sentence='she is a good student .'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cô ấy là một sinh viên tốt.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ohk3nnPRIm8s",
        "outputId": "170bcf6d-c994-4665-829a-6b328a18f140"
      },
      "source": [
        "sentence='my father is going to shopping.'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cha tôi sẽ mua sắm.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "867aMPJ2I5DA",
        "outputId": "fee17655-492b-4fc2-c281-66e04ea13fc7"
      },
      "source": [
        "sentence='the building was destroyed ten year ago .'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'toà nhà đã bị phá huỷ 10 năm trước.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W_QPV-nLJY30",
        "outputId": "520ab53b-3630-4cd6-b733-df59307965a0"
      },
      "source": [
        "sentence='she have a lot of oranges.'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cô ấy có rất nhiều cam.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LnmkdJKKLAiW",
        "outputId": "9aa6fd49-48ea-4913-c183-e159ecd99dd8"
      },
      "source": [
        "sentence='i have booked a hotel for 3 person .'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tôi đã đăng ký một khách sạn 3 người.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}